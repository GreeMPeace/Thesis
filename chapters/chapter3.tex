\chapter{System Architecture}
\section{System Requirements}
\subsection{Hardware Requirements}
One of the most important hardware elements for the three dimensional visualization is the graphics~card\footnote{In some cases the hardware acceleration is disabled. See Section \ref{SoftReq} for further information.}. Depending on the size and level of detail of the scene there are a lot of computations that need to be done. Furthermore, the scene is supposed to be movable, so it cannot be a prerendered image. That makes some dedicated graphics hardware nearly indispensable. However, since the application renders simple polygon meshes and point clouds the graphics hardware does not have to be a high end device. It merely needs to relieve the CPU of some work. And with its processing unit made especially for matrix and vector calculation, any modern day graphics card that supports WebGL will meet the expectations.

CPU-only rendering is of course possible, too. The large amount of of points and surfaces in a complex scene however seriously slows down the rendering on an all purpose CPU. This also slows down the whole system, because of all the graphics calculations that are blocking the CPU. For a more detailed analysis see Section \ref{test}.

Another important hardware requirement is the RAM. Visualization data sets, especially from simulation applications, can get large very fast. City wide simulations with multiple antennas and a resolution of a few meters can easily have a few million data points. While the data that is being visualized might not take up much space as a file on the hard drive, within the application that might change. After being loaded and parsed, the data is stored within convenient data structures like arrays or objects. This leads to a less efficient compression of the data and also the addressing schemes and object methods add additional overhead. All that together results in an application that needs to load big chunks of data into the RAM. Therefor it is important that the system has enough memory at its disposal.
\subsection{Software Requirements} \label{SoftReq}
Since the main routine of the application, the rendering loop, runs in a web browser as JavaScript code, it is important to have a browser, that supports all the used functionality. Firstly, and most importantly, the renderer used here is a so called ``webGL-renderer'', so it is important that the browser even supports WebGL. All current versions of the commonly used ones (e.g. Chrome, Firefox, Safari, \ldots) are able to support HTML5, CSS3 and WebGL. It is advisable to use the most recent update of the browser, because the developers always improve the performance or fix bugs. Also the support of WebGL grew over time, so very old versions may not support all the features used in this application. For lesser known browsers the functionality has to be determined individually. Important criteria are the former mentioned HTML5, CSS3 and WebGL support. It is also worth mentioning that the browser needs to allow WebGL to use hardware accelerated rendering, in other word access the graphics card. Microsoft's Internet Explorer for example does not do that. This leads to the problem, that it uses CPU-only rendering even if a graphics card is installed, which in turn, slows down the whole system.
\section{Architectural Design}
In this section I describe the evolution of the project, starting with the implementation of the core functionality. Afterwards, I continue to explain the split between the rendering and the data management.
\subsection{Initial JavaScript code}
After choosing the Three.js to be the framework for the graphics calculations, I made the first steps towards a full application within a simple JavaScript document. Using a very basic HTML document that just executes the script, the work started.

It was clear from the beginning, that some form of file system access was needed. First, the application used the simple HTML5 \emph{<input>}-tag with the file-type attribute. That way, for debugging purposes, files could be chosen freely by the developer an were displayed immediately afterwards. The application parsed the files, saved the processed contents into global variables and set a flag variable. The rendering loop checked the flags in every run and loaded any new data. For that, there are special routines corresponding to the type of data. They process and convert it into different objects, that can be displayed by Three.js.

This approach of handling the data turned out to be helpful in debugging the core functionality. However, when it comes to real usability it has more disadvantages than advantages. While it is more user-friendly to just have to open a simple HTML document, the way of personally choosing the files to be visualized every time one restarts the application is very unhandy. Furthermore, any further file system access is impossible, since JavaScript is not allowed to simply do that for obvious security reasons. Moreover, the reading an processing of the data takes some time for big data sets, which causes the GUI to freeze until the data routine is finished. Since there is no easy way to implement multitasking in JavaScript, there is no easy solution for this problem.

All this reasons led to the conclusion, that a JavaScript only code cannot satisfy the demands I put towards the application. Therefore, I decided to divide the functionality and create a local web app.
\subsection{Frontend -- Backend}
In order to make the GUI work more smoothly I needed to divide between the graphical display and the data acquisition. Everything concerning the graphics stayed in JavaScript code, since that is where WebGL and Three.js can be used. The file system access and rudimentary data processing however could be outsourced into a small web server backend.

The frontend stayed the same for the most part. Only the functions and classes that handled the different input files had to be updated. Those now used ajax requests provided by the JQuery~\cite{test} library. Ajax requests are a way to send a request to the corresponding web server without having to reload the whole page. In other words, the application is able to load new data in the background, while keeping on rendering the present scene.

The backend did not exist until now, so it had to be designed from the beginning. For convenience this application uses an ASP.Net server backend which is written in C\#. This server backend architecture was designed by Microsoft and therefore the Microsoft Visual Studio IDE provides substantial support and many templates for it. Since this thesis and the application are not about web development, this was a easy and quick way to implement the intended functionality. However, the backend is not very big or demanding so if another implementation (e.g. in Ruby) should proof to be more convenient in the future, a transition would be relatively easy.

The design of the backend recreates part of a widely known web programming conventions called MVC. In the MVC model the server uses three different templates to represent different abstractions. The models encapsulate different datastructures. They provide a general representation for each type of data, which in many cases come from databases. The view is a visual representation of the result. It gets delivered to the browser and can be displayed. Finally the Controller is the core of the server. A Controller gets called if the respective request comes in. The controller uses the data in the form of models to create a view, that gets delivered back. Since this application generates its own visual representation the ``view'' -part can be neglected. Furthermore, there is no data that would call for the use of a database, so the models are simple classes, mimicking the datastructures that the frontend expects. So all in all, when a request comes in from the frontend, it gets routed to a Controller, depending on its URL. The controller then reads the files in question and insert the contents into the right models, which in turn get delivered back to the frontend.

